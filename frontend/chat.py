import os
import requests
import streamlit as st
from dotenv import load_dotenv
from typing import Dict, List, Optional
import json
import sseclient

load_dotenv(override=True)

st.set_page_config(
    page_title="AIå¯¹è¯",
    page_icon="ğŸ’¬",
    layout="wide"
)

API_URL = os.getenv("API_URL", "http://localhost:8001")
CONFIG_FILE = "model_config.json"

MODEL_PROVIDERS = {
    "OpenAI": {
        "name": "OpenAI",
        "api_key_env": "OPENAI_API_KEY",
        "base_url_env": "OPENAI_BASE_URL",
        "default_base_url": "https://api.openai.com/v1",
        "models": [
            "gpt-5.1",
            "gpt-5.2",
        ],
        "requires_base_url": False
    },
    "AzureOpenAI": {
        "name": "Azure OpenAI",
        "api_key_env": "AZURE_OPENAI_API_KEY",
        "endpoint_env": "AZURE_OPENAI_ENDPOINT",
        "deployment_env": "AZURE_OPENAI_DEPLOYMENT",
        "api_version_env": "AZURE_OPENAI_API_VERSION",
        "default_api_version": "2024-02-15-preview",
        "models": [
            "gpt-5.1",
            "gpt-5.2",
        ],
        "requires_endpoint": True
    },
    "ç¡…åŸºæµåŠ¨": {
        "name": "ç¡…åŸºæµåŠ¨",
        "api_key_env": "SILICONFLOW_API_KEY",
        "base_url_env": "SILICONFLOW_BASE_URL",
        "default_base_url": "https://api.siliconflow.cn/v1",
        "models": [
            "deepseek-ai/DeepSeek-V3",
            "deepseek-ai/DeepSeek-R1",
            "Qwen/Qwen2.5-72B-Instruct",
            "THUDM/glm-4-9b-chat",
            "meta-llama/Llama-3.3-70B-Instruct",
            "01-ai/Yi-1.5-34B-Chat"
        ],
        "requires_base_url": False
    },
    "é˜¿é‡Œç™¾ç‚¼": {
        "name": "é˜¿é‡Œç™¾ç‚¼",
        "api_key_env": "DASHSCOPE_API_KEY",
        "base_url_env": "DASHSCOPE_BASE_URL",
        "default_base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "models": [
            "qwen-max",
            "qwen-plus",
            "qwen-turbo",
            "qwen-long",
            "qwen-vl-max",
            "qwen-vl-plus"
        ],
        "requires_base_url": False
    },
    "DeepSeek": {
        "name": "DeepSeek",
        "api_key_env": "DEEPSEEK_API_KEY",
        "base_url_env": "DEEPSEEK_BASE_URL",
        "default_base_url": "https://api.deepseek.com/v1",
        "models": [
            "deepseek-chat",
            "deepseek-coder",
            "deepseek-reasoner"
        ],
        "requires_base_url": False
    }
}

def validate_model_config(provider: str, api_key: str, base_url: Optional[str] = None, endpoint: Optional[str] = None, api_version: Optional[str] = None) -> tuple[bool, str]:
    try:
        provider_config = MODEL_PROVIDERS[provider]
        
        if not api_key:
            return False, f"{provider_config['name']} API Key ä¸èƒ½ä¸ºç©º"
        
        if provider == "AzureOpenAI":
            if not endpoint:
                return False, "Azure OpenAI Endpoint ä¸èƒ½ä¸ºç©º"
            if not api_version:
                return False, "Azure OpenAI API Version ä¸èƒ½ä¸ºç©º"
        else:
            if provider_config.get("requires_base_url", False) and not base_url:
                return False, f"{provider_config['name']} Base URL ä¸èƒ½ä¸ºç©º"
        
        return True, "é…ç½®éªŒè¯é€šè¿‡"
    except Exception as e:
        return False, f"éªŒè¯å¤±è´¥: {str(e)}"

def test_model_connection(provider: str, model: str, api_key: str, base_url: Optional[str] = None, endpoint: Optional[str] = None, api_version: Optional[str] = None) -> tuple[bool, str]:
    try:
        if provider == "AzureOpenAI":
            url = f"{endpoint.rstrip('/')}/openai/deployments/{model}/chat/completions?api-version={api_version}"
            headers = {"api-key": api_key, "Content-Type": "application/json"}
            payload = {
                "messages": [{"role": "user", "content": "test"}]
            }
        else:
            url = f"{base_url.rstrip('/')}/chat/completions"
            headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
            payload = {
                "model": model,
                "messages": [{"role": "user", "content": "test"}]
            }
        
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        
        if response.status_code == 200:
            return True, "è¿æ¥æµ‹è¯•æˆåŠŸ"
        elif response.status_code == 401:
            return False, "API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ"
        elif response.status_code == 404:
            return False, "æ¨¡å‹ä¸å­˜åœ¨æˆ–æœªéƒ¨ç½²"
        elif response.status_code == 429:
            return False, "è¯·æ±‚é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•"
        else:
            error_data = response.json() if response.headers.get("content-type", "").startswith("application/json") else {}
            error_msg = error_data.get("error", {}).get("message", f"HTTP {response.status_code}")
            return False, f"è¿æ¥å¤±è´¥: {error_msg}"
    except requests.exceptions.Timeout:
        return False, "è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æœåŠ¡åœ°å€"
    except requests.exceptions.ConnectionError:
        return False, "æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æœåŠ¡åœ°å€"
    except Exception as e:
        return False, f"æµ‹è¯•è¿æ¥æ—¶å‡ºé”™: {str(e)}"

def fetch_available_models(provider: str, api_key: str, base_url: Optional[str] = None, endpoint: Optional[str] = None, api_version: Optional[str] = None) -> tuple[bool, List[str], str]:
    try:
        if provider == "AzureOpenAI":
            url = f"{endpoint.rstrip('/')}/openai/deployments?api-version={api_version}"
            headers = {"api-key": api_key}
        else:
            url = f"{base_url.rstrip('/')}/models"
            headers = {"Authorization": f"Bearer {api_key}"}
        
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            
            if provider == "AzureOpenAI":
                models = [deployment.get("model", deployment.get("id", "")) for deployment in data.get("data", [])]
            else:
                models = [model.get("id", "") for model in data.get("data", [])]
            
            models = [m for m in models if m]
            
            if models:
                return True, models, f"æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹"
            else:
                return False, [], "æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹"
        else:
            return False, [], f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}"
    except Exception as e:
        return False, [], f"è·å–æ¨¡å‹åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}"

def send_chat_message(messages, top_k=5, use_hybrid=True, use_query_expansion=True, model_config=None):
    try:
        payload = {
            "messages": messages,
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ä¼˜å…ˆåŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·åŸºäºè”ç½‘æœç´¢ç»“æœå›ç­”ã€‚å›ç­”æ—¶è¯·æ¸…æ™°æ ‡æ³¨ä¿¡æ¯æ¥æºã€‚",
            "top_k": top_k,
            "use_hybrid": use_hybrid,
            "use_query_expansion": use_query_expansion
        }
        
        if model_config:
            payload["llm_config"] = model_config
        
        response = requests.post(
            f"{API_URL}/chat",
            json=payload,
            timeout=120
        )
        if response.status_code == 200:
            return response.json()
        return None
    except Exception as e:
        return None

def send_chat_message_stream(messages, top_k=5, use_hybrid=True, use_query_expansion=True, model_config=None):
    try:
        payload = {
            "messages": messages,
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ä¼˜å…ˆåŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·åŸºäºè”ç½‘æœç´¢ç»“æœå›ç­”ã€‚å›ç­”æ—¶è¯·æ¸…æ™°æ ‡æ³¨ä¿¡æ¯æ¥æºã€‚",
            "top_k": top_k,
            "use_hybrid": use_hybrid,
            "use_query_expansion": use_query_expansion
        }
        
        if model_config:
            payload["llm_config"] = model_config
        
        response = requests.post(
            f"{API_URL}/chat/stream",
            json=payload,
            stream=True,
            timeout=300
        )
        
        if response.status_code == 200:
            client = sseclient.SSEClient(response)
            
            metadata = None
            error = None
            
            for event in client.events():
                try:
                    data = json.loads(event.data)
                    
                    if data.get("type") == "metadata":
                        metadata = data.get("data")
                    elif data.get("type") == "content":
                        yield data.get("content", ""), metadata
                    elif data.get("type") == "error":
                        error = data.get("error")
                        break
                    elif data.get("type") == "done":
                        break
                        
                except json.JSONDecodeError:
                    continue
            
            if error:
                yield None, {"error": error}
        else:
            yield None, {"error": f"HTTP {response.status_code}"}
    except Exception as e:
        yield None, {"error": str(e)}

def save_model_config(config: Dict):
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        return True, "é…ç½®å·²ä¿å­˜"
    except Exception as e:
        return False, f"ä¿å­˜é…ç½®å¤±è´¥: {str(e)}"

def load_model_config() -> Optional[Dict]:
    try:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    except Exception:
        return None

def render_model_config_sidebar():
    st.sidebar.markdown("### ğŸ¤– æ¨¡å‹é…ç½®")
    
    if "model_config" not in st.session_state:
        saved_config = load_model_config()
        if saved_config:
            st.session_state.model_config = saved_config
        else:
            st.session_state.model_config = {
                "provider": "OpenAI",
                "model": "gpt-4o-mini",
                "api_key": "",
                "base_url": "",
                "endpoint": "",
                "api_version": "",
                "temperature": 0.7
            }
    
    provider_options = list(MODEL_PROVIDERS.keys())
    provider_display = [MODEL_PROVIDERS[p]["name"] for p in provider_options]
    
    selected_provider_display = st.sidebar.selectbox(
        "é€‰æ‹©ä¾›åº”å•†",
        options=provider_display,
        index=provider_options.index(st.session_state.model_config["provider"]),
        help="é€‰æ‹©è¦ä½¿ç”¨çš„AIæ¨¡å‹ä¾›åº”å•†",
        key="provider_selectbox"
    )
    
    selected_provider = provider_options[provider_display.index(selected_provider_display)]
    
    if st.session_state.model_config["provider"] != selected_provider:
        st.session_state.model_config["provider"] = selected_provider
        provider_config = MODEL_PROVIDERS[selected_provider]
        api_key_env_value = os.getenv(provider_config["api_key_env"], "")
        st.session_state.model_config["api_key"] = api_key_env_value
        if selected_provider == "AzureOpenAI":
            endpoint_env_value = os.getenv(provider_config["endpoint_env"], "")
            api_version_env_value = os.getenv(provider_config["api_version_env"], "")
            deployment_env_value = os.getenv(provider_config["deployment_env"], "")
            st.session_state.model_config["endpoint"] = endpoint_env_value
            st.session_state.model_config["api_version"] = api_version_env_value or provider_config["default_api_version"]
            st.session_state.model_config["deployment"] = deployment_env_value
            st.session_state.model_config["base_url"] = ""
        else:
            base_url_env_value = os.getenv(provider_config.get("base_url_env", ""), "")
            st.session_state.model_config["base_url"] = base_url_env_value or provider_config.get("default_base_url", "")
            st.session_state.model_config["endpoint"] = ""
            st.session_state.model_config["api_version"] = ""
            st.session_state.model_config["deployment"] = ""
        st.session_state.model_config["model"] = provider_config["models"][0]
        st.rerun()
    
    provider_config = MODEL_PROVIDERS[selected_provider]
    
    st.sidebar.caption(f"å½“å‰ä¾›åº”å•†: {provider_config['name']}")
    
    api_key_env_value = os.getenv(provider_config["api_key_env"], "")
    api_key = st.sidebar.text_input(
        "API Key",
        value=st.session_state.model_config.get("api_key") or api_key_env_value,
        type="password",
        help=f"è¾“å…¥ {provider_config['name']} çš„ API Keyï¼ˆä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡ {provider_config['api_key_env']} é…ç½®ï¼‰"
    )
    st.session_state.model_config["api_key"] = api_key
    
    if selected_provider == "AzureOpenAI":
        endpoint_env_value = os.getenv(provider_config["endpoint_env"], "")
        endpoint = st.sidebar.text_input(
            "Endpoint",
            value=st.session_state.model_config.get("endpoint") or endpoint_env_value,
            help="Azure OpenAI æœåŠ¡ç«¯ç‚¹ï¼Œä¾‹å¦‚: https://your-resource.openai.azure.com"
        )
        st.session_state.model_config["endpoint"] = endpoint
        
        api_version_env_value = os.getenv(provider_config["api_version_env"], "")
        api_version = st.sidebar.text_input(
            "API Version",
            value=st.session_state.model_config.get("api_version") or api_version_env_value or provider_config["default_api_version"],
            help="Azure OpenAI API ç‰ˆæœ¬"
        )
        st.session_state.model_config["api_version"] = api_version
        
        deployment_env_value = os.getenv(provider_config["deployment_env"], "")
        deployment = st.sidebar.text_input(
            "éƒ¨ç½²åç§° (å¯é€‰)",
            value=st.session_state.model_config.get("deployment") or deployment_env_value,
            help="Azure OpenAI éƒ¨ç½²åç§°ï¼ˆå¦‚æœä¸æ¨¡å‹åç§°ä¸åŒï¼‰"
        )
        st.session_state.model_config["deployment"] = deployment
    else:
        base_url_env_value = os.getenv(provider_config.get("base_url_env", ""), "")
        base_url = st.sidebar.text_input(
            "Base URL (å¯é€‰)",
            value=st.session_state.model_config.get("base_url") or base_url_env_value or provider_config.get("default_base_url", ""),
            help=f"{provider_config['name']} API çš„ Base URLï¼ˆç•™ç©ºä½¿ç”¨é»˜è®¤å€¼ï¼‰"
        )
        st.session_state.model_config["base_url"] = base_url
    
    available_models = provider_config["models"]
    
    # if st.sidebar.button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨", key="refresh_models"):
    #     with st.spinner("æ­£åœ¨è·å–æ¨¡å‹åˆ—è¡¨..."):
    #         if selected_provider == "AzureOpenAI":
    #             success, models, message = fetch_available_models(
    #                 selected_provider, api_key,
    #                 endpoint=st.session_state.model_config.get("endpoint"),
    #                 api_version=st.session_state.model_config.get("api_version")
    #             )
    #         else:
    #             success, models, message = fetch_available_models(
    #                 selected_provider, api_key,
    #                 base_url=st.session_state.model_config.get("base_url")
    #             )
            
    #         if success:
    #             st.sidebar.success(f"âœ“ {message}")
    #             if models:
    #                 MODEL_PROVIDERS[selected_provider]["models"] = models
    #                 st.session_state.model_config["model"] = models[0]
    #                 st.rerun()
    #         else:
    #             st.sidebar.error(f"âœ— {message}")
    
    if st.session_state.model_config["model"] not in available_models:
        st.session_state.model_config["model"] = available_models[0]
    
    selected_model = st.sidebar.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        options=available_models,
        index=available_models.index(st.session_state.model_config["model"]),
        help=f"é€‰æ‹© {provider_config['name']} çš„å…·ä½“æ¨¡å‹"
    )
    st.session_state.model_config["model"] = selected_model
    
    st.sidebar.markdown("### ğŸ›ï¸ é«˜çº§å‚æ•°")
    temperature = st.sidebar.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=st.session_state.model_config.get("temperature", 0.7),
        step=0.1,
        help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¾“å‡ºè¶Šéšæœº"
    )
    st.session_state.model_config["temperature"] = temperature
    
    col1, col2, col3 = st.sidebar.columns(3)
    with col1:
        if st.sidebar.button("âœ… éªŒè¯", use_container_width=True):
            is_valid, message = validate_model_config(
                selected_provider,
                api_key,
                base_url=st.session_state.model_config.get("base_url"),
                endpoint=st.session_state.model_config.get("endpoint"),
                api_version=st.session_state.model_config.get("api_version")
            )
            if is_valid:
                st.sidebar.success(f"âœ“ {message}")
            else:
                st.sidebar.error(f"âœ— {message}")
    
    with col2:
        if st.sidebar.button("ğŸ”— æµ‹è¯•", use_container_width=True):
            is_valid, _ = validate_model_config(
                selected_provider,
                api_key,
                base_url=st.session_state.model_config.get("base_url"),
                endpoint=st.session_state.model_config.get("endpoint"),
                api_version=st.session_state.model_config.get("api_version")
            )
            if not is_valid:
                st.sidebar.error("âœ— è¯·å…ˆå®Œæˆé…ç½®éªŒè¯")
            else:
                with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
                    if selected_provider == "AzureOpenAI":
                        success, message = test_model_connection(
                            selected_provider, selected_model, api_key,
                            endpoint=st.session_state.model_config.get("endpoint"),
                            api_version=st.session_state.model_config.get("api_version")
                        )
                    else:
                        success, message = test_model_connection(
                            selected_provider, selected_model, api_key,
                            base_url=st.session_state.model_config.get("base_url")
                        )
                    if success:
                        st.sidebar.success(f"âœ“ {message}")
                    else:
                        st.sidebar.error(f"âœ— {message}")
    
    with col3:
        if st.sidebar.button("ğŸ’¾ ä¿å­˜", use_container_width=True):
            success, message = save_model_config(st.session_state.model_config)
            if success:
                st.sidebar.success(f"âœ“ {message}")
            else:
                st.sidebar.error(f"âœ— {message}")
    
    st.sidebar.divider()
    
    return st.session_state.model_config

st.title("ğŸ’¬ AIæ™ºèƒ½å¯¹è¯")

st.sidebar.title("âš™ï¸ è®¾ç½®")

model_config = render_model_config_sidebar()

is_config_valid, _ = validate_model_config(
    model_config["provider"],
    model_config.get("api_key", ""),
    base_url=model_config.get("base_url"),
    endpoint=model_config.get("endpoint"),
    api_version=model_config.get("api_version")
)

if not is_config_valid:
    st.warning("âš ï¸ æ¨¡å‹é…ç½®ä¸å®Œæ•´ï¼Œè¯·åœ¨ä¾§è¾¹æ å®Œæˆé…ç½®åä½¿ç”¨")

st.sidebar.markdown("### ğŸ” æ£€ç´¢è®¾ç½®")
top_k = st.sidebar.slider(
    "æ£€ç´¢å—æ•°é‡",
    min_value=1,
    max_value=20,
    value=5,
    step=1,
    help="ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢çš„ç›¸ä¼¼æ–‡æœ¬å—æ•°é‡"
)
st.sidebar.caption("ğŸ’¡ è¾ƒå¤šçš„æ£€ç´¢å—å¯ä»¥æä¾›æ›´å…¨é¢çš„ä¿¡æ¯ï¼Œä½†å¯èƒ½å¢åŠ å“åº”æ—¶é—´")

st.sidebar.markdown("### ğŸ”„ æ··åˆæ£€ç´¢è®¾ç½®")
use_hybrid = st.sidebar.checkbox(
    "å¯ç”¨æ··åˆæ£€ç´¢ (å‘é‡ + BM25)",
    value=True,
    help="ç»“åˆå‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢ï¼Œæé«˜æ£€ç´¢å‡†ç¡®æ€§"
)

use_query_expansion = st.sidebar.checkbox(
    "å¯ç”¨æŸ¥è¯¢æ‰©å±•",
    value=True,
    help="è‡ªåŠ¨ç”Ÿæˆè¯­ä¹‰ç›¸è¿‘çš„æŸ¥è¯¢å˜ä½“ï¼Œæé«˜å¬å›ç‡"
)

st.sidebar.divider()

if st.sidebar.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯"):
    st.session_state.chat_messages = []
    st.rerun()

st.sidebar.divider()

st.markdown("""
<div style="padding: 10px; background-color: #e3f2fd; border-radius: 5px; margin-bottom: 20px;">
    <strong>ğŸ’¡ ä½¿ç”¨è¯´æ˜ï¼š</strong>ç³»ç»Ÿä¼šä¼˜å…ˆæ£€ç´¢çŸ¥è¯†åº“ä¸­çš„å†…å®¹å›ç­”æ‚¨çš„é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå°†è‡ªåŠ¨è”ç½‘æœç´¢è·å–ç­”æ¡ˆã€‚
</div>
""", unsafe_allow_html=True)

if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []

for message in st.session_state.chat_messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        if message.get("role") == "assistant":
            if message.get("used_knowledge"):
                st.info("ğŸ“š åŸºäºçŸ¥è¯†åº“å›ç­”")
                if message.get("knowledge_sources"):
                    with st.expander("ğŸ“– çŸ¥è¯†æ¥æº", expanded=False):
                        for i, source in enumerate(message["knowledge_sources"], 1):
                            st.markdown(f"**æ¥æº {i}** (ç›¸å…³åº¦: {source['score']:.2f})")
                            st.markdown(f"{source['content']}")
                            st.markdown(f"*æ¥è‡ª: {source['filename']}*")
                            st.divider()
            elif message.get("search_results"):
                st.info("ğŸŒ åŸºäºè”ç½‘æœç´¢å›ç­”")
                with st.expander("ğŸ” æœç´¢æ¥æº", expanded=False):
                    for result in message["search_results"]:
                        st.markdown(f"- [{result.get('title', 'æ— æ ‡é¢˜')}]({result.get('url', '')})")

if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.chat_messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("ğŸ¤” æ€è€ƒä¸­...")
        
        is_valid, validation_msg = validate_model_config(
            model_config["provider"],
            model_config.get("api_key", ""),
            base_url=model_config.get("base_url"),
            endpoint=model_config.get("endpoint"),
            api_version=model_config.get("api_version")
        )
        
        if not is_valid:
            message_placeholder.markdown(f"âŒ æ¨¡å‹é…ç½®é”™è¯¯: {validation_msg}\n\nè¯·åœ¨ä¾§è¾¹æ é…ç½®æ¨¡å‹åé‡è¯•ã€‚")
        else:
            assistant_response = ""
            metadata = None
            error = None
            
            for content, meta in send_chat_message_stream(st.session_state.chat_messages, top_k, use_hybrid, use_query_expansion, model_config):
                if meta and "error" in meta:
                    error = meta["error"]
                    break
                if meta:
                    metadata = meta
                if content:
                    assistant_response += content
                    message_placeholder.markdown(assistant_response)
            
            if error:
                message_placeholder.markdown(f"âŒ å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {error}")
            elif assistant_response:
                search_results = metadata.get("search_results") if metadata else None
                knowledge_sources = metadata.get("knowledge_sources") if metadata else None
                used_knowledge = metadata.get("used_knowledge") if metadata else False
                
                if used_knowledge:
                    st.info("ğŸ“š åŸºäºçŸ¥è¯†åº“å›ç­”")
                    if knowledge_sources:
                        with st.expander("ğŸ“– çŸ¥è¯†æ¥æº", expanded=False):
                            for i, source in enumerate(knowledge_sources, 1):
                                st.markdown(f"**æ¥æº {i}** (ç›¸å…³åº¦: {source['score']:.2f})")
                                st.markdown(f"{source['content']}")
                                st.markdown(f"*æ¥è‡ª: {source['filename']}*")
                                st.divider()
                elif search_results:
                    st.info("ğŸŒ åŸºäºè”ç½‘æœç´¢å›ç­”")
                    with st.expander("ğŸ” æœç´¢æ¥æº", expanded=False):
                        for result in search_results:
                            st.markdown(f"- [{result.get('title', 'æ— æ ‡é¢˜')}]({result.get('url', '')})")
                
                st.session_state.chat_messages.append({
                    "role": "assistant",
                    "content": assistant_response,
                    "search_results": search_results,
                    "knowledge_sources": knowledge_sources,
                    "used_knowledge": used_knowledge
                })
            else:
                message_placeholder.markdown("âŒ æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚")
